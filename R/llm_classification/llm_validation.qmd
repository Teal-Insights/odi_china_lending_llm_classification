---
title: "LLM Validation"
author: "Teal Emery"
format: html
editor: visual
---

# Setup

## Load Libraries

```{r}
library(tidyverse)
library(ellmer)
library(here)
library(chinadevfin3)
library(beepr)
library(progressr)
library(jsonlite)
```

## Source Scripts

```{r}
source(
  here(
    "R",
    "llm_classification",
    "llm_functions.R"
  )
)
```

## System Prompt

```{r}
system_prompt_txt <- here(
  "R",
  "llm_classification",
  "classification-prompt.md"
) |> 
  read_lines() |> 
  paste(collapse = "\n")

system_prompt_txt
```

## Get GCDF Data

```{r}
gcdf_data_for_llm <- get_gcdf3_dataset() |> 
  filter(
    recommended_for_aggregates == "Yes"
  ) |> 
   select(
    aid_data_record_id,
    sector_name,
    title,
    description,
    staff_comments
    ) |> 
  mutate(
    combined_text = paste(
      "# Sector\n", sector_name, "\n\n",
      "# Title\n", title, "\n\n",
      "# Description\n", description, "\n\n",
      "# Staff Comments\n", staff_comments,
      sep = ""
    )
  ) 

gcdf_data_for_llm
```

## Load Llama

```{r}
# ollamar::pull("llama3.3")
```

```{r}
coal_example <- gcdf_data_for_llm |> 
  filter(aid_data_record_id == 92546) |> 
  pull(combined_text)

coal_example
```

```{r}
# chat <- chat_ollama(
#   system_prompt = system_prompt_txt,
#   model = "llama3.3"
# )
# 
# result <- chat$extract_data(coal_example, type = project_analysis_spec)
# 
# result 
```

## Test OpenAI API

```{r}
chat <- chat_openai(
  system_prompt = system_prompt_txt,
  model = "gpt-4o-mini"
)

oai_result <- chat$extract_data(coal_example, type = project_analysis_spec)

oai_result
```

## Test Anthropic API

```{r}
chat <- chat_claude(
  system_prompt = system_prompt_txt,
  model = "claude-3-5-sonnet-20241022"
)

claude_result <- chat$extract_data(coal_example, type = project_analysis_spec)

claude_result 
```

## Test Llama 3.3-70B API

```{r}
chat <- chat_vllm(
  base_url = "https://promptly-technologies-llc--example-vllm-openai-compatible-serve.modal.run/v1/",
  system_prompt = system_prompt_txt,
  model = "Llama-3.3-70B-Instruct"
)

llama_result <- chat$chat(coal_example)

# Remove triple backticks and optional "json"
llama_result <- gsub("^```(json)?\\s*", "", llama_result)

# Remove trailing triple backticks
llama_result <- gsub("```$", "", llama_result)

# Parse the JSON string
llama_result <- fromJSON(llama_result)

llama_result
```

# Run 300 Examples

## Create Sample

```{r}
set.seed(42) # For reproducibility

gcdf_sample_for_validation <- gcdf_data_for_llm |>
  filter(
    # sectors likely to have high density of Green/Brown/Grey projects
    sector_name %in% c(
      "ENERGY",
      "TRANSPORT AND STORAGE",
      "INDUSTRY, MINING, CONSTRUCTION"
    )
  ) |> 
  slice_sample(n = 300)

gcdf_sample_for_validation
```

## Save Sample

```{r}
write_csv(
  gcdf_sample_for_validation,
  here(
    "data",
    "gcdf_sample_for_llm_validation.csv"
  )
)
```

### LLama 3.3

```{r}
# start_time <- Sys.time()
# llama_results <- process_all_chunks(
#   gcdf_sample_for_validation,
#   chunk_size = 50,
#   chat_provider = "ollama",
#   output_dir = "data/validation_results",
#   run_name = "llm_validation"
# )
# end_time <- Sys.time()
# beep(3)
# 
# # How long did it take?
# llama_results_time_elapsed <- end_time - start_time
# llama_results_time_elapsed
# 
# llama_results_time_elapsed_per_loan <- gpt_4o_mini_time_elapsed/300
# llama_results_time_elapsed_per_loan
# 
# # Collect results
# llama_results_run_results <- collect_results(
#   "data/validation_results", 
#   chat_provider = "ollama",
#   run_id = "llm_validation"
# )
```

### GPT 4o-mini

```{r}
start_time <- Sys.time()
results <- process_all_chunks(
  gcdf_sample_for_validation,
  chunk_size = 10,
  chat_provider = "openai-gpt-4o-mini",
  output_dir = "data/validation_results",
  run_name = "llm_validation"
)
end_time <- Sys.time()
beep(3)

# How long did it take?
gpt_4o_mini_time_elapsed <- end_time - start_time
gpt_4o_mini_time_elapsed

gpt_4o_mini_time_elapsed_per_loan <- gpt_4o_mini_time_elapsed/300
gpt_4o_mini_time_elapsed_per_loan

# Collect results
gpt_4o_mini_run_results <- collect_results(
  "data/validation_results", 
  chat_provider = "openai-gpt-4o-mini",
  run_id = "llm_validation"
)
```

```{r}
gpt_4o_mini_run_results$fa
```

### GPT 4o-mini

```{r}
start_time <- Sys.time()
results <- process_all_chunks(
  gcdf_sample_for_validation,
  chunk_size = 10,
  chat_provider = "claude",
  output_dir = "data/validation_results",
  run_name = "llm_validation"
)
end_time <- Sys.time()
beep(3)

# How long did it take?
claude_time_elapsed <- end_time - start_time
claude_time_elapsed

claude_time_elapsed_per_loan <- claude_time_elapsed/300
claude_time_elapsed_per_loan

# Collect results
claude_run_results <- collect_results(
  "data/validation_results", 
  chat_provider =  "claude",
  run_id = "llm_validation"
)
```

```{r}
claude_run_results
```

```{r}
df_with_tokens <- gcdf_data_for_llm %>%
  mutate(
    char_count = str_length(combined_text),
    approx_tokens = char_count / 4
  )

df_with_tokens %>%
  summarize(
    avg_tokens = mean(approx_tokens),
    percentile_75 = quantile(approx_tokens, probs = 0.75),
    max_tokens = max(approx_tokens)
  )
```

```{r}
str_length(system_prompt_txt)/4
```