---
title: "LLM Validation"
author: "Teal Emery"
format: html
editor: visual
---

# Setup

## Load Libraries

```{r}
library(tidyverse)
library(ellmer)
library(here)
library(chinadevfin3)
library(beepr)
library(progressr)
library(jsonlite)
```

## Source Scripts

```{r}
source(
  here(
    "R",
    "llm_classification",
    "llm_functions.R"
  )
)
```

## System Prompt

```{r}
system_prompt_txt <- here(
  "R",
  "llm_classification",
  "classification-prompt.md"
) |> 
  read_lines() |> 
  paste(collapse = "\n")

system_prompt_txt
```

## Get GCDF Data

```{r}
gcdf_data_for_llm <- get_gcdf3_dataset() |> 
  filter(
    recommended_for_aggregates == "Yes"
  ) |> 
   select(
    aid_data_record_id,
    sector_name,
    title,
    description,
    staff_comments
    ) |> 
  mutate(
    combined_text = paste(
      "# Sector\n", sector_name, "\n\n",
      "# Title\n", title, "\n\n",
      "# Description\n", description,
      sep = ""
    )
  ) 

gcdf_data_for_llm
```

## Load Llama

```{r}
# ollamar::pull("llama3.3")
```

```{r}
coal_example <- gcdf_data_for_llm |> 
  filter(aid_data_record_id == 92546) |> 
  pull(combined_text)

coal_example
```

```{r}
chat <- chat_ollama(
  system_prompt = system_prompt_txt,
  model = "llama3.3"
)

result <- chat$extract_data(coal_example, type = project_analysis_spec)

result
```

## Test OpenAI API

```{r}
chat <- chat_openai(
  system_prompt = system_prompt_txt,
  model = "gpt-4o-mini"
)

oai_result <- chat$extract_data(coal_example, type = project_analysis_spec)

oai_result
```

## Test Anthropic API

```{r}
chat <- chat_claude(
  system_prompt = system_prompt_txt,
  model = "claude-3-5-sonnet-20241022"
)

claude_result <- chat$extract_data(coal_example, type = project_analysis_spec)

claude_result 
```

## Test Llama 3.3-70B API

```{r}
chat <- chat_vllm(
  base_url = "https://promptly-technologies-llc--example-vllm-openai-compatible-serve.modal.run/v1/",
  system_prompt = system_prompt_txt,
  model = "Llama-3.3-70B-Instruct"
)

llama_result <- chat$chat(coal_example)

# Remove triple backticks and optional "json"
llama_result <- gsub("^```(json)?\\s*", "", llama_result)

# Remove trailing triple backticks
llama_result <- gsub("```$", "", llama_result)

# Parse the JSON string
llama_result <- fromJSON(llama_result)

llama_result
```

## Test DeepSeek V3

```{r}
chat <- chat_vllm(
  base_url = "https://api.deepseek.com/v1/",
  system_prompt = system_prompt_txt,
  model = "deepseek-chat",
  api_key=Sys.getenv("DEEPSEEK_API_KEY")
)

deepseek_result <- chat$chat(coal_example)

deepseek_result
```

```{r}
chat <- chat_vllm(
  base_url = "https://api.deepseek.com/v1/",
  system_prompt = paste(
    system_prompt_txt,
    "\nIMPORTANT: You must return valid JSON without any markdown formatting.",
    sep = "\n"
  ),
  model = "deepseek-chat",
  api_key = Sys.getenv("DEEPSEEK_API_KEY"),
  api_args = list(response_format = list(type = "json_object"))
)

deepseek_result <- chat$extract_data(coal_example, type = project_analysis_spec)
```

```{r}
process_deepseek_response <- function(response) {
  # Remove everything between and including ```json and ``` 
  clean_text <- gsub("```json\\s*\\{", "{", response)
  clean_text <- gsub("\\s*```\\s*$", "", clean_text)
  
  tryCatch({
    # Parse the JSON into a list structure
    parsed <- jsonlite::fromJSON(clean_text, simplifyVector = TRUE)
    
    # Convert into simple vectors where appropriate
    list(
      classification = list(
        primary = as.character(parsed$classification$primary),
        confidence = as.character(parsed$classification$confidence),
        project_type = as.character(parsed$classification$project_type)
      ),
      justification = as.character(parsed$justification),
      evidence = as.character(parsed$evidence)
    )
  }, error = function(e) {
    cli::cli_abort(
      c("Failed to parse DeepSeek response",
        i = "Original response: {response}",
        x = "Error: {conditionMessage(e)}")
    )
  })
}

chat <- chat_vllm(
  base_url = "https://api.deepseek.com/v1/",
  system_prompt = paste(
    system_prompt_txt,
    "\nIMPORTANT: Return ONLY raw JSON without any markdown code blocks, backticks, or formatting. The response should start with '{' and end with '}'.",
    sep = "\n"
  ),
  model = "deepseek-chat",
  api_key = Sys.getenv("DEEPSEEK_API_KEY")
)

deepseek_result <- chat$chat(coal_example, echo = FALSE) |> 
  process_deepseek_response()

# Print result
print(deepseek_result)
```
deepseek_result

# Run 300 Examples

## Create Sample

```{r}
set.seed(42) # For reproducibility

gcdf_sample_for_validation <- gcdf_data_for_llm |>
  filter(
    # sectors likely to have high density of Green/Brown/Grey projects
    sector_name %in% c(
      "ENERGY",
      "TRANSPORT AND STORAGE",
      "INDUSTRY, MINING, CONSTRUCTION"
    )
  ) |> 
  slice_sample(n = 300)

gcdf_sample_for_validation
```

## Save Sample

```{r}
write_csv(
  gcdf_sample_for_validation,
  here(
    "data",
    "gcdf_sample_for_llm_validation.csv"
  )
)
```

### LLama 3.3

```{r}
# start_time <- Sys.time()
# llama_results <- process_all_chunks(
#   gcdf_sample_for_validation,
#   chunk_size = 10,
#   chat_provider = "ollama",
#   output_dir = "data/validation_results",
#   run_name = "llm_validation"
# )
# end_time <- Sys.time()
# beep(3)
# 
# # How long did it take?
# llama_results_time_elapsed <- end_time - start_time
# llama_results_time_elapsed
# 
# llama_results_time_elapsed_per_loan <- llama_results_time_elapsed/300
# llama_results_time_elapsed_per_loan
# 
# # Collect results
# llama_results_run_results <- collect_results(
#   "data/validation_results",
#   chat_provider = "ollama",
#   run_id = "llm_validation"
# )
```

```{r}

```

### GPT 4o-mini

```{r}
# start_time <- Sys.time()
# results <- process_all_chunks(
#   gcdf_sample_for_validation,
#   chunk_size = 10,
#   chat_provider = "openai-gpt-4o-mini",
#   output_dir = "data/validation_results",
#   run_name = "llm_validation"
# )
# end_time <- Sys.time()
# beep(3)
# 
# # How long did it take?
# gpt_4o_mini_time_elapsed <- end_time - start_time
# gpt_4o_mini_time_elapsed
# 
# gpt_4o_mini_time_elapsed_per_loan <- gpt_4o_mini_time_elapsed/300
# gpt_4o_mini_time_elapsed_per_loan
# 
# # Collect results
# gpt_4o_mini_run_results <- collect_results(
#   "data/validation_results", 
#   chat_provider = "openai-gpt-4o-mini",
#   run_id = "llm_validation"
# )
```

```{r}
# gpt_4o_mini_run_results$fa
```

### Claude

```{r}
# start_time <- Sys.time()
# results <- process_all_chunks(
#   gcdf_sample_for_validation,
#   chunk_size = 10,
#   chat_provider = "claude",
#   output_dir = "data/validation_results",
#   run_name = "llm_validation"
# )
# end_time <- Sys.time()
# beep(3)
# 
# # How long did it take?
# claude_time_elapsed <- end_time - start_time
# claude_time_elapsed
# 
# claude_time_elapsed_per_loan <- claude_time_elapsed/300
# claude_time_elapsed_per_loan
# 
# # Collect results
# claude_run_results <- collect_results(
#   "data/validation_results", 
#   chat_provider =  "claude",
#   run_id = "llm_validation"
# )
```

```{r}
claude_run_results
```

## Deepseek
```{r}
deepseek_test <- gcdf_sample_for_validation |> 
  head(3)

deepseek_test
```



```{r}
start_time <- Sys.time()
results <- process_all_chunks(
  gcdf_sample_for_validation,
  chunk_size = 10,
  chat_provider = "deepseek",  
  output_dir = "data/validation_results",
  run_name = "llm_validation"
)
end_time <- Sys.time()
#beep(3)

# # How long did it take?
deepseek_time_elapsed <- end_time - start_time
deepseek_time_elapsed

deepseek_time_elapsed_per_loan <- deepseek_time_elapsed/300
deepseek_time_elapsed_per_loan

# Collect results
deepseek_run_results <- collect_results(
  "data/validation_results",
  chat_provider =  "deepseek",
  run_id = "llm_validation"
)
```

```{r}
deepseek_time_elapsed_per_loan * 18000 / 60
```

```{r}
deepseek_run_results
```

